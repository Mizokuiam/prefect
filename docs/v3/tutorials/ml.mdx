---
title: Train a machine learning model
description: Use an automation to trigger an ML training flow whenever data changes
---

In the [Extract data from websites](/v3/tutorials/scraping) tutorial, you learned how to handle data dependencies and ingest large amounts of data.
Now, you'll learn how to train a machine learning model using that data.

## Create a GPU work pool

Run the following command to automatically provision an ECS push work pool and supporting ECS infrastructure on AWS:

```bash
prefect work-pool create --type ecs:push --provision-infra my-ecs-pool
```

Now create a `task_definition.json` file with the following task definition which the work pool will use to provision `g4dn.xlarge` instances on EC2.
TODO: Do we need to specify a Amazon ECS GPU-optimized AMI separately anywhere?
TODO: Create a new Docker image that has the necessary dependencies (python, pip, prefect[aws]) and use that image instead of the base nvidia/cuda one.

```json task_definition.json
{
  "family": "gpu-task",
  "requiresCompatibilities": [
    "EC2"
  ],
  "networkMode": "awsvpc",
  "cpu": "4096",
  "memory": "16384",
  "containerDefinitions": [
    {
      "name": "gpu-container",
      "image": "nvidia/cuda:latest",
      "cpu": 4096,
      "memory": 16384,
      "essential": true,
      "resourceRequirements": [
        {
          "type": "GPU",
          "value": "1"
        }
      ],
      "linuxParameters": {
        "devices": [
          {
            "hostPath": "/dev/nvidia0",
            "containerPath": "/dev/nvidia0",
            "permissions": [
              "read",
              "write"
            ]
          }
        ]
      }
    }
  ],
  "placementConstraints": [
    {
      "type": "memberOf",
      "expression": "attribute:instance-type == g4dn.xlarge"
    }
  ]
}
```

Command to register the task definition in `us-east-1` using the AWS CLI:

```bash
aws ecs register-task-definition --cli-input-json file://task_definition.json --region us-east-1
```

Finally, edit the `my-ecs-pool` work pool definition in Prefect Cloud and set the **Base Job Template** / **Task Definition Arn** setting to the ARN of the task definition you just registered.

You'll also need 

## Store training data in S3

TBD.
Show the scraping code, but give people file(s) they can upload to S3 (original data plus changes to trigger the flow).

## Train a model when the training data changes

Prefect configuration:

* Create two Prefect blocks:
  * An `AWS Credentials` block named `aws-credentials`
  * An `S3 Bucket` block named `s3-bucket`
* Create a Prefect webhook named `s3-webhook`
  * Create a new service account named `s3` to enforce webhook authentication (save the API key somewhere safe)
  * Create a static template preset with the default settings (event named `webhook.called`)
* Create a Prefect automation named `s3-automation`
  * Trigger settings
    * Trigger on any `webhook.called` event that comes from the `s3-webhook` webhook resource that you created.
  * Trigger whenever you see that event 1 time within 0 day (this way your model will only trained whenever you see the event)
  * Action settings:
    * Action type: `Run a deployment`
    * Deployment to run: `update-model > webhook-test`
    * path: `test.txt`

AWS configuration:

* Create a new S3 bucket named `prefect-tutorial`
  * Under **Properties** > **Amazon EventBridge**, turn on the option to send notifications to EventBridge for all events in this bucket.
* Create a new EventBridge rule that sends an event to the Prefect webhook whenever an object is created in the `prefect-tutorial` bucket.
  * You'll need to use the URL and credentials from the webhook that you created earlier.
    * Create a new Connection and configure its authorization to use **API Key** and set the **API key name** to `Authorization` and the **Value** to `Bearer <your-api-key>`.

Code configuration:

* Clone the Prefect demo repository
* Run `python train_model.py` to deploy the flow to Prefect Cloud

## Test the model

* Run `prefect deployment run 'update-model/webhook-test'` to run the flow manually (fetching `test.txt` from S3)
* Upload a new file to S3 to run the flow automatically using the webhook you set up
* Once the flow is complete, go to artifacts to see the model's performance metrics

The model is now trained and uploaded to S3 for your inference needs.

## Next steps

In this tutorial, you learned how to publish data to S3 and train a machine learning model whenever that data changes.
You're well on your way to becoming a Prefect expert!

Now that you've finished this tutorial series, continue your learning journey by going deep on the following topics:

- Write [flows](/v3/develop/write-flows) and [tasks](/v3/develop/write-tasks)
- Manage [Prefect Cloud and server instances](/v3/manage)
- Run workflows on [work pools](/v3/deploy/infrastructure-concepts/work-pools) using Kubernetes, Docker, and serverless infrastructure.

<Tip>
Need help? [Book a meeting](https://calendly.com/prefect-experts/prefect-product-advocates?utm_campaign=prefect_docs_cloud&utm_content=prefect_docs&utm_medium=docs&utm_source=docs) with a Prefect Product Advocate to get your questions answered.
</Tip>
